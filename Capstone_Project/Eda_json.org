* EDA - Correct Data
:properties:
:header-args:ein-python: :session http://127.0.0.1:8888/EDA.ipynb
:end: 

** Global Paramters

 #+NAME: A4F2CD83-A7EA-4661-B3DF-4EF6F32D3161
 #+begin_src ein-python :results output
WRKDIR = "/Users/marcohassan/Desktop/Learning/AI_workflow_Coursera/Capstone_Project/"
DATADIR = WRKDIR + "ai-workflow-capstone/cs-train/"
 #+end_src

** Libraries

Multiple important packages

#+NAME: 0D5B735B-F781-4732-809C-983E1992B501
#+begin_src ein-python :results output
#!START_LIB
## To deal with files and OS
import os
import sys
import shutil

## Regular Expression
import re

## To dump the esimated model
import pickle

## To automatically set a default data type to an unspecified key in a
## dict.
from collections import defaultdict

## For date manipulation
import time
from datetime import datetime

## Standard data manipulation libraries
import numpy as np
import pandas as pd

## For sql in Pandas
from pandasql import sqldf
nba = lambda q: sqldf(q, globals())

## For plotting
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from pandas.plotting import register_matplotlib_converters
import seaborn as sns

register_matplotlib_converters()

## for statistics evaluation; mathematical python package.
from scipy import stats
import math
#!END_LIB
#+end_src

** Import functions

Import the python package with your user-defined functions.

#+NAME: 86334714-1029-4884-8D04-74AE85380BB5
#+begin_src ein-python :results output
import functions
#+end_src

** Display Options

#+NAME: 0438BC5E-80DD-4BD1-A107-1AF4F21277B8
#+begin_src ein-python :results output
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)

np.set_printoptions(threshold=np.inf)
#+end_src

** ETL - Extract, Transform and Load

#+NAME: F6FF40D4-81CF-4C08-BDA9-30D37927D5CA
#+BEGIN_SRC ein-python  :results output
df = functions.fetch_data (DATADIR)
#+END_SRC

#+RESULTS: F6FF40D4-81CF-4C08-BDA9-30D37927D5CA
#+begin_example
Fetching ..../Users/marcohassan/Desktop/Learning/AI_workflow_Coursera/Capstone_Project/ai-workflow-capstone/cs-train/invoices-2018-01.json

Fetching ..../Users/marcohassan/Desktop/Learning/AI_workflow_Coursera/Capstone_Project/ai-workflow-capstone/cs-train/invoices-2017-12.json

Fetching ..../Users/marcohassan/Desktop/Learning/AI_workflow_Coursera/Capstone_Project/ai-workflow-capstone/cs-train/invoices-2019-01.json

Fetching ..../Users/marcohassan/Desktop/Learning/AI_workflow_Coursera/Capstone_Project/ai-workflow-capstone/cs-train/invoices-2018-11.json

Fetching ..../Users/marcohassan/Desktop/Learning/AI_workflow_Coursera/Capstone_Project/ai-workflow-capstone/cs-train/invoices-2018-07.json

Fetching ..../Users/marcohassan/Desktop/Learning/AI_workflow_Coursera/Capstone_Project/ai-workflow-capstone/cs-train/invoices-2019-06.json

Fetching ..../Users/marcohassan/Desktop/Learning/AI_workflow_Coursera/Capstone_Project/ai-workflow-capstone/cs-train/invoices-2019-07.json

Fetching ..../Users/marcohassan/Desktop/Learning/AI_workflow_Coursera/Capstone_Project/ai-workflow-capstone/cs-train/invoices-2018-06.json

Fetching ..../Users/marcohassan/Desktop/Learning/AI_workflow_Coursera/Capstone_Project/ai-workflow-capstone/cs-train/invoices-2018-10.json

Fetching ..../Users/marcohassan/Desktop/Learning/AI_workflow_Coursera/Capstone_Project/ai-workflow-capstone/cs-train/invoices-2018-09.json

Fetching ..../Users/marcohassan/Desktop/Learning/AI_workflow_Coursera/Capstone_Project/ai-workflow-capstone/cs-train/invoices-2019-04.json

Fetching ..../Users/marcohassan/Desktop/Learning/AI_workflow_Coursera/Capstone_Project/ai-workflow-capstone/cs-train/invoices-2018-05.json

Fetching ..../Users/marcohassan/Desktop/Learning/AI_workflow_Coursera/Capstone_Project/ai-workflow-capstone/cs-train/invoices-2018-12.json

Fetching ..../Users/marcohassan/Desktop/Learning/AI_workflow_Coursera/Capstone_Project/ai-workflow-capstone/cs-train/invoices-2018-04.json

Fetching ..../Users/marcohassan/Desktop/Learning/AI_workflow_Coursera/Capstone_Project/ai-workflow-capstone/cs-train/invoices-2019-05.json

Fetching ..../Users/marcohassan/Desktop/Learning/AI_workflow_Coursera/Capstone_Project/ai-workflow-capstone/cs-train/invoices-2018-08.json

Fetching ..../Users/marcohassan/Desktop/Learning/AI_workflow_Coursera/Capstone_Project/ai-workflow-capstone/cs-train/invoices-2019-02.json

Fetching ..../Users/marcohassan/Desktop/Learning/AI_workflow_Coursera/Capstone_Project/ai-workflow-capstone/cs-train/invoices-2018-03.json

Fetching ..../Users/marcohassan/Desktop/Learning/AI_workflow_Coursera/Capstone_Project/ai-workflow-capstone/cs-train/invoices-2017-11.json
Fetching ..../Users/marcohassan/Desktop/Learning/AI_workflow_Coursera/Capstone_Project/ai-workflow-capstone/cs-train/invoices-2018-02.json

Fetching ..../Users/marcohassan/Desktop/Learning/AI_workflow_Coursera/Capstone_Project/ai-workflow-capstone/cs-train/invoices-2019-03.json
#+end_example

** EDA 

*** Check for missing entries

 #+NAME: 50B66596-AACB-4CC9-9D9D-13DC221A3AAE
 #+begin_src ein-python :results output
print ("Data Types\n{}\n".format ("="* 35))
print (df.dtypes) 

print ("\nNull Values\n{}\n".format ("="* 35))
print (df.isnull().sum(axis = 0))
 #+end_src

#+NAME: 6830D2FD-47F3-4FAE-BF51-060BC5F6200E
#+begin_src ein-python :results output
df.head (5)
#+end_src

#+RESULTS: 6830D2FD-47F3-4FAE-BF51-060BC5F6200E
:           country customer_id  day invoice  month  price stream_id  times_viewed  year invoice_date
: 0  United Kingdom       17998   28  489562     11   5.95     84656             1  2017   2017-11-28
: 1  United Kingdom       13085   28  489434     11   6.75    79323W            12  2017   2017-11-28
: 2  United Kingdom       13085   28  489434     11   2.10     22041            21  2017   2017-11-28
: 3  United Kingdom       13085   28  489434     11   1.25     21232             5  2017   2017-11-28
: 4  United Kingdom       13085   28  489434     11   1.65     22064            17  2017   2017-11-28

*** Split Categorical And Numeric Variables

#+NAME: 2F3340EE-0B78-4A8A-9F24-42EF7477215E
#+begin_src ein-python :results output
## get categorical variables
num_variables = list (df.describe ().columns) 

cat_variables = [x for x in df.columns if x not in num_variables]

print("Numerical Variables\n{}\n{}\n\nCategorical Variables\n{}\n{}" \
      .format("="*35, num_variables, "="*35, cat_variables))
#+end_src


Print Number of categories

#+NAME: 87137DFB-BC0F-497B-B8FB-53ABDB765BAF
#+begin_src ein-python :results output
for i in cat_variables: 
    print ("Number Different {}: {}\n".format (i, len (df[i].unique())))

print (f"Total Dataset size: {len (df)}") 
#+end_src

You can notice above that some of the variables are wrongly
classified. Day, month, year should be treated as categories. On top
of it also customer_id should clearly be treated as a category.

*** Replace the missing customer_id


#+NAME: 35BCA4A4-FA09-49A8-B975-52989A1DE152
#+BEGIN_SRC ein-python  :results output
df.columns
#+END_SRC

#+NAME: 1C0AFFC8-4090-4E46-B5A0-65ED33A714D1
#+begin_src ein-python :results output
df.customer_id[df.customer_id.isnull()].head (5)
#+end_src

#+RESULTS: 1C0AFFC8-4090-4E46-B5A0-65ED33A714D1
: Series([], Name: customer_id, dtype: object)

#+NAME: 7E3FCBC7-0DCA-45EB-92E2-EDAD33F1AFB9
#+begin_src ein-python :results output
df.loc[df.customer_id[df.customer_id.isnull()].index, "customer_id"] = 'noID'
#+end_src

#+NAME: 7FD36E6A-8629-4716-917D-4636BFEB6491
#+begin_src ein-python :results output
df.isnull ().sum (axis = 0)
#+end_src

*** 10 Countries with higher sales

#+NAME: A8870111-C29E-4D93-911A-19B7C8F19B25
#+BEGIN_SRC ein-python  :results output
df[["country", "price"]].groupby (df["country"]).sum ().sort_values (by = "price", ascending = False).index[:10]
#+END_SRC

#+RESULTS: A8870111-C29E-4D93-911A-19B7C8F19B25
: 10

 #+NAME: 57015F06-859F-466A-8DE6-EA0D92FB8A7D
 #+begin_src ein-python :results output
q = \
"""
SELECT country, sum(price) as tot_revenue
FROM df 
GROUP BY country
ORDER BY tot_revenue DESC
"""
 #+end_src
   
#+NAME: B98227B2-FBE2-4B07-87E2-BE9093270D83
#+BEGIN_SRC ein-python  :results output
df.columns
#+END_SRC

 #+NAME: E14A7CBB-FE59-4A7B-BF7E-98A0F3DE5F20
 #+begin_src ein-python :results output
a = nba (q)
 #+end_src

 Store the countries with the highest revenue

 #+NAME: 0E1E9464-E10F-435D-9C0A-2EFADFF53E14
 #+begin_src ein-python :results output
max_countries = np.array(a.country.head(10))
 #+end_src

*** Restrict the dataset to the relevant 10 countries for the next visualization exercise

#+NAME: 0CC003D9-4A74-4540-9C43-68A6862B36B1
#+begin_src ein-python :results output
print (f"Share of captured data: {df.country.map (lambda x: x in max_countries).sum (axis = 0)/len (df)}")

df_max_country = df[df.country.map (lambda x: x in max_countries)]
#+end_src

*** Convert all the necessary variables to categorical variables

#+NAME: 7CAAF612-2735-4922-83F9-79B826FA92F2
#+begin_src ein-python :results output
to_convert = [x for x in num_variables if x not in ['price', 'times_viewed', 'month']]

df_max_country[to_convert] = df_max_country[to_convert].astype ('category')
df_max_country[cat_variables] = df_max_country[cat_variables].astype ('category')

cat_dtype = pd.api.types.CategoricalDtype(
   categories=[x for x in range(1,13)], ordered=True)

df_max_country["month"] = df_max_country.month.astype ('int').astype(cat_dtype)
#+end_src
 
#+NAME: 29BDCABA-7F49-41E7-AB6E-BB2563C76969
#+begin_src ein-python :results output
df_max_country.dtypes
#+end_src

*** Visualization
    
#+NAME: 6576AC4E-2CF2-43B0-9DE4-2ADD25829499
#+begin_src ein-python :results output
q = \
"""
SELECT country, year, month, SUM(price) as revenue
FROM df_max_country
GROUP BY country, year, month
"""
#+end_src

#+NAME: 2FBDE7BE-2D80-4BB9-8ECB-051B0B5B4400
#+begin_src ein-python :results output
df_rev_year = nba (q)
#+end_src

#+NAME: 505CE1C9-6AA6-423B-866F-5CD08D1A5E08
#+begin_src ein-python 
k1 = sns.violinplot(data = df_rev_year, x = 'year', y = 'revenue') 
#+end_src


Notice that above the width represents the amount of observations as
interaction of months and country that fall in the specific bin.

From the above you can see that the distribution of the revenues is
highly skewed.

#+NAME: 746E160E-A785-4966-AD43-B63E155790CC
#+begin_src ein-python :results output
k1 = sns.violinplot(data = df_rev_year, x = 'year', y = 'revenue', hue = "month") 
#+end_src

Notice moreover that you do not have entries for each month in every
year

** Convert to Times Series the Series for the Top 10 countries

#+NAME: DEC93A22-FBE9-44F3-B271-E257148A2B3D
#+begin_src ein-python :results output
df_aggregate = functions.convert_df_to_ts (df, max_countries)
#+end_src

*** Visualize the aggregated data

 #+NAME: AA9EFEBA-58EA-4A0E-83DE-CF47F6458F8C
 #+begin_src ein-python :results output
print (df_aggregate.dtypes) 
print ()
print (df_aggregate.head (5))
 #+end_src

Check the distribution and relation among the variables

#+NAME: B764FCA0-CF28-4F48-84C5-D9EF99B2E259
#+begin_src ein-python :results output
sns.set(style="ticks", color_codes=True)

## make a pair plot
g = sns.PairGrid(df_aggregate[['purchases', 'unique_invoices',
                               'total_views',
                               'unique_streams', 'country']],
                 hue = 'country')
g = g.map_upper(sns.scatterplot, alpha = 0.3)
g = g.map_lower(sns.kdeplot)
g = g.map_diag(plt.hist, lw=2, alpha = 0.2).add_legend()
#+end_src

n[[file:ein-images/ob-ein-7acdeade1676a91a92d672aa07232032.png]]

Notice that the sample is highly dominated by the United Kingdom. In
fact we can see that this makes up a consistent share of the total
revenues

#+NAME: D2F680C6-441C-49F5-AB32-42B5F370C759
#+begin_src ein-python :results output
a = df_aggregate["country"] == "United Kingdom"

df_aggregate[~a][["revenue"]].sum()/df_aggregate[["revenue"]].sum() * 100 
#+end_src

#+NAME: 0256D68B-F130-4687-9495-9B8723DB0428
#+begin_src ein-python :results output
df_aggregate[["revenue", "country"]].groupby ("country").count ()
#+end_src

*** Look at the correlation structure

#+NAME: DC2CDFE4-C637-4DBC-AE34-9627B74734ED
#+begin_src ein-python :results output
corr = df_aggregate.corr()

# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(corr, dtype=np.bool))
# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))
# Generate a custom diverging colormap
cmap = sns.diverging_palette(220, 10, as_cmap=True)
# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
	    square=True, linewidths=.5, cbar_kws={"shrink": .5})
#+end_src

[[file:ein-images/ob-ein-388e0643eb2e641234f4b02c32f268a6.png]]


As you could expect all of the variables correlates with one another.

*** Hypothesis 

    - No sensible difference among the countries in the average year.

    - No statistical significant trend in the series.


Notice that the revenues looks as being log-normal distributed

#+NAME: 92AB40A5-0931-4084-AC9A-E6E3621DE3A5
#+begin_src ein-python :results output
tot_countries = len (df_aggregate.country.astype('category').cat.categories)
#+end_src


#+NAME: ACA2AB65-285F-4486-BD8E-4AC270039B69
#+begin_src ein-python :results output
## specify your R par(mfrow=c(x,y))
f, axes = plt.subplots(figsize = (12, 6))

idx = 1

for country in max_countries:
    plt.subplot (2, tot_countries/2, idx)
    k1 = sns.distplot(df_aggregate[df_aggregate.country == country][["revenue"]], 
                      kde = False ,fit = stats.lognorm)
    k1.title.set_text(country)
    idx += 1

plt.show ()
#+end_src

[[file:ein-images/ob-ein-a219c53734d648b70a33b9b20ae607cf.png]]


Test the hypothesis that the revenues are log-normal for each country.

#+NAME: B491C417-C187-4A16-A76D-0C8CCB36C036
#+begin_src ein-python :results output
for country in max_countries:
    a = np.array(df_aggregate[df_aggregate.country == country][["revenue"]])

    a[a == 0]  = 10^-10

    log_values = np.log(a)

    print("For country {} the p-value for a log-normal distribution is: {} \n".format(country, stats.jarque_bera(a)[1]))
#+end_src

So the hypothesis is rejected with quite strong confidence. 

*** Needed Data

    Ideal dataset:

    - continum of data. not disconnected as in the provided dataset
      where just for 1 single year we have observations in each of the
      months. It becomes diffcult to extract seasonality from it. 

#+NAME: 456B8EE2-2BE9-47CF-B1CB-08F3D8443D47
#+begin_src ein-python :results output
for test_country in max_countries:

    print("{}\n{}".format(test_country, "="*35))
    
    for country in max_countries:
        a = np.array(df_aggregate[df_aggregate.country == test_country][["revenue"]])

        b = np.array(df_aggregate[df_aggregate.country == country][["revenue"]])

        tStat, pValue = stats.ttest_ind(a, b, equal_var = False) #run independent sample T-Test

        print("P-value of equal revenues for {}: {}".format(country, pValue))

    print() 
#+end_src

    - more granular level. for instance the type of sold items.

    - more contextual data such as country economic indicators,
      demographics etc. Like this you might be able to analyze
      non-intuitive correlation structure.

** Feature Engineering and Model Estimation

#+NAME: 6E39B5DA-FD3D-4E4D-A84D-1673670DB386
#+begin_src ein-python :results output
# features_mat = functions.engineer_features_by_country(df_aggregate,
#                                                       training = 0,
#                                                       countries = max_countries)

# If using this function you would then have to run the analysis by
# country. Keep it simple here. It's a demo for workflow and not focus
# on best modeling.

features_mat = functions.engineer_features(df_aggregate,
                                           training = 0)
#+end_src

Given the above it is now possible to estimate the revenues in the
next month. Either for the different countries separately merging then
in a final step the results or directly at the aggregated level.

#+NAME: 7E090F65-4579-4565-98E0-A0A9642E6774
#+begin_src ein-python :results output
features_mat[0].dtypes
#+end_src

Notice that recent invoices and recent views were computed based on
the average value of the =unique_views= and =total_views= for the
period.

Try some basic training technique

#+NAME: 73BB109D-3F71-48B1-909F-C25ADB416A1B
#+begin_src ein-python :results output
#!START_LIB
from sklearn.model_selection import train_test_split, GridSearchCV

from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.impute import SimpleImputer

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

from sklearn.preprocessing import StandardScaler, OneHotEncoder

# Regression Models
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor

from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
#!END_LIB
#+end_src


For the exercise we first forget about the structural dependency in
the data. I just map a set of features to the output via standard
regression techniques.  

#+NAME: 61D9705F-3EFC-4A53-AA2C-06B41686239B
#+begin_src ein-python :results output
## If using functions.engineer_features_by_country ()
# X_train, X_test, y_train, y_test = train_test_split(features_mat[[x for x in features_mat.columns if x != 'y_revenue']], 
#                                                     np.array(features_mat[['y_revenue']]),
#                                                     test_size = 0.3,
#                                                     shuffle = False,
# 						    random_state = 1)

X_train, X_test, y_train, y_test = train_test_split(features_mat[0], 
                                                    features_mat[1],
                                                    test_size = 0.3,
                                                    shuffle = False,
						    random_state = 1)
#+end_src

#+RESULTS: 61D9705F-3EFC-4A53-AA2C-06B41686239B

Notice the importance of *not shuffling here*. This due to the times
series structure.

#+NAME: 0037EF68-223D-4A5C-9DBC-EE93049E556A
#+begin_src ein-python :results output
print (y_test[:10])

# print (features_mat[['y_revenue']][int (len (features_mat)*0.7) :int(len (features_mat)*0.7) + 10])

print (features_mat[1][int (len (features_mat[1])*0.7) :int(len (features_mat[1])*0.7) + 10])
#+end_src


#+NAME: 1D19C3E2-4A46-40F6-9B4D-C295DDC9535B
#+begin_src ein-python :results output
ex2 = [3.89, 5.78,
       7.42086181, 9.42086181,
       2.1904, 6.1966,
       1.7743]
#+end_src

#+RESULTS: 1D19C3E2-4A46-40F6-9B4D-C295DDC9535B


#+NAME: 6881DF88-CF16-4E63-A504-8EE989C86957
#+begin_src ein-python :results output
query = np.array(ex2)

print (query.shape) 

query = query.reshape(1, -1)

query = pd.DataFrame(query)

query.columns = ['previous_7', 'previous_14',
		 'previous_28', 'previous_70',
		 'previous_year', 'recent_invoices',
		 'recent_views']

clf.predict (query)
#+end_src

#+RESULTS: 6881DF88-CF16-4E63-A504-8EE989C86957
: (7,)
: 
: array([182494.164])


*** Regression Exercise - Not Considering Temporal Structure in the Data 

**** Create a simple baseline estimator. 

  Compute a baseline estimator to check if the modeling made sense.

  #+NAME: 1F2983EC-B46F-4A65-9760-6962B9D8308B
  #+begin_src ein-python :results output
print("Baseline - Simple Mean \n{}\n\n Mean Squared Error: {} \n Mean Absolute Error : {}". \
      format("=" * 35, 
             mean_squared_error(y_test, np.repeat(np.mean(y_train), y_test.size)), 
             mean_absolute_error(y_test, np.repeat(np.mean(y_train), y_test.size))
             )
      )
  #+end_src

  So just a marginal improvement over a very rough baseline

**** More Complex Models

  #+NAME: 45E1D510-734E-4903-B0A5-413C15080F45
  #+begin_src ein-python :results output
## preprocessing pipeline
cat_features = [x for x in X_train.columns if x not in X_train.describe().columns]
num_features = list(X_train.describe().columns)
  #+end_src

  #+RESULTS: 45E1D510-734E-4903-B0A5-413C15080F45

  #+NAME: 9AD78777-0C13-4170-B30F-B25E60197E80
  #+begin_src ein-python :results output
numeric_transformer = Pipeline(steps=[
   ('imputer', SimpleImputer(strategy='mean')),
   ('scaler', StandardScaler())])

categorical_transformer = Pipeline(steps=[
   ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
   ('onehot', OneHotEncoder(handle_unknown='ignore'))])

preprocessor = ColumnTransformer(
   transformers=[
       ('num', numeric_transformer, num_features),
       ('cat', categorical_transformer, cat_features)])
  #+end_src

  #+RESULTS: 9AD78777-0C13-4170-B30F-B25E60197E80

**** Support Vector Regression

  Estimation Pipeline

  #+NAME: 17194981-3BF6-40A4-8B1E-9BFE5E6B0DA9
  #+begin_src ein-python :results output
param_grid_svm = {
      'svr__C': [0.01,0.1,0.5,1.0,1.5,5.0,10.0],
}

pipe_svr = Pipeline(steps = 
                    [
                        ('pre', preprocessor),
                        ('svr', SVR(kernel = 'rbf'))
                    ])

grid = GridSearchCV(pipe_svr,
		    param_grid=param_grid_svm,
		    cv=5)

grid.fit(X_train, y_train)

y_pred = grid.predict(X_test)

best_params = grid.best_params_
  #+end_src

  Check at the summary statistics

  #+NAME: 50C3A21E-78C8-4118-84B8-E14EFBC8B001
  #+begin_src ein-python :results output
print("SVR \n{}\n\n Mean Squared Error: {} \n Mean Absolute Error : {}". \
      format("=" * 35, 
             mean_squared_error(y_test, y_pred), 
             mean_absolute_error(y_test, y_pred)
             )
      )
  #+end_src

 Not working at all. Even worsening the performance. 

**** Random Forest

  #+NAME: 96444CFC-F47E-4F80-8C46-9B03CBC25A33
 #+begin_src ein-python :results output
param_grid_tree = {
      'dtree__max_depth': [4,5,6,7,8,9,10,12,14,15],
}

pipe_dtree = Pipeline(steps = 
                    [
                        ('pre', preprocessor),
                        ('dtree', DecisionTreeRegressor())
                    ])

grid = GridSearchCV(pipe_dtree,
		    param_grid=param_grid_tree,
		    cv=5)

grid.fit(X_train, y_train)

y_pred = grid.predict(X_test)

best_params = grid.best_params_
  #+end_src

  #+RESULTS: 96444CFC-F47E-4F80-8C46-9B03CBC25A33

#+NAME: F4F5730B-37CD-4F31-BD45-4A5D56C2206B
#+BEGIN_SRC ein-python  :results output
# features_mat[0].dtypes
y_pred = grid.predict(X_test)
#+END_SRC

#+RESULTS: F4F5730B-37CD-4F31-BD45-4A5D56C2206B


#+NAME: 6EA1716C-7DC9-4D84-B8FA-88EE9CDCF653
#+begin_src ein-python :results output
np.array (X_test.iloc[0,:]) 
#+end_src

#+RESULTS: 6EA1716C-7DC9-4D84-B8FA-88EE9CDCF653
: array([3.18669700e+04, 5.83556410e+04, 1.42086181e+05, 3.53433941e+05,
:        9.06022210e+04, 6.19666667e+00, 5.77430000e+02])


  #+NAME: 80D4F982-F09B-4DB6-B60D-737D8FFA3255
  #+begin_src ein-python :results output
print("Random Forest \n{}\n\n Mean Squared Error: {} \n Mean Absolute Error : {}". \
      format("=" * 35, 
             mean_squared_error(y_test, y_pred), 
             mean_absolute_error(y_test, y_pred)
             )
      )
  #+end_src

  #+RESULTS: 80D4F982-F09B-4DB6-B60D-737D8FFA3255
  : Random Forest 
  : ===================================
  : 
  :  Mean Squared Error: 319662.2463548565 
  :  Mean Absolute Error : 95.2118754989748

  Sensible improvement. Possibly strong non-linearities captured by
  the decision Tree Regressor.

*** TODO Classical Times Series Models

    This are skipped here at this stage. These models you have
    extensively studied at Uni. Focus on the new.

*** Try a multivariate LSTM model

 Notice that LSTM accepts 3D arrays arguments

 #+NAME: DB53B5FA-8760-47FC-8D08-922B38DD9F7B
 #+BEGIN_SRC ein-python  :results output
features_mat[0].columns
 #+END_SRC

 Notice that here I remove categorical variables as keras does not know
 how to deal out of the box with them. You should hot_encode them
 manually or put them in a numeric fashion if you want to incorporate
 them.

 #+NAME: 07C62635-5E5E-4516-A087-AE50839625D5
 #+begin_src ein-python :results output
# X_train, X_test, y_train, y_test = train_test_split(features_mat[[x for x in features_mat.columns if x not in ['y_revenue', 
#                                                                                                                'country',
#                                                                                                                'date']]], 
#                                                     np.array(features_mat[['y_revenue']]),
#                                                     test_size = 0.3,
#                                                     shuffle = False,
# 						    random_state = 1)
 #+end_src


 #+NAME: 2848C2B2-5218-4A0C-B6AA-C4AB5AEEF7B8
 #+begin_src ein-python :results output
# reshape input to be 3D [samples, timesteps, features]
X_train = np.array (X_train) .reshape((X_train.shape[0], 1, X_train.shape[1]))
X_test = np.array (X_test).reshape((X_test.shape[0], 1, X_test.shape[1]))
print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)
 #+end_src

 #+NAME: 0E93D24B-37B0-4BD3-80E3-DFDE6E8F3B8C
 #+BEGIN_SRC ein-python  :results output
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.models import Sequential
 #+END_SRC

 #+NAME: 50F65769-C2E5-4EA6-ABD7-10FF0A2DD15B
 #+begin_src ein-python :results output
# design network
model = Sequential()
model.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2])))
model.add(Dense(1))
model.compile(loss='mae', optimizer='adam')
# fit network
history = model.fit(X_train, y_train, epochs=100, batch_size=142, validation_data=(X_test, y_test), verbose=2, shuffle=False)
 #+end_src

 #+NAME: DA9C7EB4-EE8F-42AC-AE5E-F6B6A0EA6D22
 #+begin_src ein-python :results output
model.compile(loss='mse', optimizer='adam')
history2 = model.fit(X_train, y_train, epochs=100, batch_size=142, validation_data=(X_test, y_test), verbose=2, shuffle=False)
 #+end_src

 #+NAME: C8A10AC4-B512-4466-948E-C7FCD723B43F
 #+begin_src ein-python :results output
print("LSTM \n{}\n\n Mean Squared Error: {} \n Mean Absolute Error : {}". \
      format("=" * 35, 
             history2.history['val_loss'][-1], 
             history.history['val_loss'][-1])
             )
 #+end_src

  : Random Forest 
  : ===================================
  : 
  :  Mean Squared Error: 319662.2463548567 
  :  Mean Absolute Error : 95.21187549897539

  : SVR 
  : ===================================
  : 
  :  Mean Squared Error: 8775565821.677137 
  :  Mean Absolute Error : 58317.993981713655

  : Baseline - Simple Mean 
  : ===================================
  : 
  :  Mean Squared Error: 798737443.6172509 
  :  Mean Absolute Error : 28195.73404389957


 So LSTM provides a benefit over the baseline. Still performs worst
 than a simple decision Forest. Given the linear improvement of the
 estimation in the epochs I assume that there is still some work to be
 done with the model before bringing this into production.


#+NAME: A8F20CC2-5048-44BC-AAA3-87B44E1260EF
#+BEGIN_SRC ein-python  :results output
grid.best_params_
#+END_SRC

 #+NAME: 3CF4092D-042F-49A0-88A9-ECFAA11B3607
 #+begin_src ein-python :results output
# plot history
plt.plot(history2.history['loss'], label='train')
plt.plot(history2.history['val_loss'], label='test')
plt.legend()
plt.show()
 #+end_src

   
** TODO 

*** add deal with missing values. simpleimputer?

*** TODO do a post about the wasserstein metric.  

    check how the movement of the mass is defined there. 

    
